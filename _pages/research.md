---
layout: page
title: Research
permalink: /research/
description: Our research focus areas and ongoing projects.
nav: true
nav_order: 2
---

# Research Areas

Our lab conducts research at the intersection of artificial intelligence and computer architecture. We focus on developing efficient systems that can handle modern AI workloads while addressing challenges in performance, energy efficiency, and scalability.

## AI Systems and Architecture

We design and optimize computing systems specifically for AI applications. This includes:

- **Hardware acceleration for deep learning**: Designing specialized hardware to speed up deep learning inference and training
- **Memory system optimization**: Improving memory hierarchies to better serve AI workloads
- **Algorithm-hardware co-design**: Developing algorithms and hardware platforms together for optimal performance

## Efficient Deep Learning

We research techniques to make deep learning models more efficient and deployable across diverse hardware platforms:

- **Neural network compression**: Reducing model size through pruning, quantization, and efficient architectural design
- **Sparse neural networks**: Exploring sparsity patterns and acceleration techniques
- **AutoML for efficient networks**: Automated design of efficient neural network architectures

## Edge AI and Embedded Systems

We explore how to deploy advanced AI capabilities on resource-constrained edge devices:

- **Tiny machine learning (TinyML)**: Running ML models on microcontrollers and low-power devices
- **Energy-efficient inference**: Techniques to minimize energy consumption for on-device inference
- **Real-time AI systems**: Meeting strict latency requirements in embedded AI applications

## Neuromorphic Computing

We investigate brain-inspired computing paradigms that may offer fundamentally different approaches to AI:

- **Spiking Neural Networks (SNNs)**: Models that more closely mimic biological neural networks
- **Emerging memory technologies**: Leveraging new non-volatile memory technologies for neuromorphic systems
- **Bio-inspired learning algorithms**: Learning approaches inspired by neuroscience

# Current Projects

Below are some of our ongoing research projects:

1. **Efficient Vision Transformers for Edge Devices**
   Developing lightweight vision transformer architectures for deployment on edge devices with limited computational resources.

2. **Memory-Centric Neural Network Acceleration**
   Exploring novel memory architectures to reduce the data movement bottleneck in neural network processing.

3. **Sparsity-Aware Computing Systems**
   Designing hardware and software techniques to exploit sparsity in neural networks for improved performance and efficiency.

4. **Neuromorphic Sensor Integration**
   Developing efficient processing systems for event-based sensors like dynamic vision sensors (DVS).

5. **Automated Neural Architecture Search for Embedded Systems**
   Creating AutoML techniques that can automatically design neural network architectures optimized for specific embedded hardware platforms. 